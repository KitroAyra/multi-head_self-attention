# multi-head_self-attention
A Faster Pytorch Implementation of Multi-Head Self-Attention
